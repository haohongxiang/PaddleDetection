{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: import ppdet from source directory without installing, run 'python setup.py install' to install ppdet firstly\n"
     ]
    }
   ],
   "source": [
    "import paddle\n",
    "import torch\n",
    "from ppdet.core.workspace import load_config, create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############patch here!!!\n"
     ]
    }
   ],
   "source": [
    "cfg = load_config('./configs/cascade_rcnn/vit_base_16_hrfpn.yml')\n",
    "model = create(cfg.architecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# state = torch.load('../params/pretrain_mae_vit_base_mask_0.75_400e.pth', map_location='cpu')['model']\n",
    "\n",
    "# state = {k.strip('encoder.'): v for k, v in state.items() if 'encoder' in k}\n",
    "\n",
    "# _state = {}\n",
    "# for n, p in model.backbone.state_dict().items():\n",
    "#     if n in state:\n",
    "#         if len(p.shape) == 2:\n",
    "#             _state[n] = state[n].data.numpy().T\n",
    "#         else:\n",
    "#             _state[n] = state[n].data.numpy()\n",
    "\n",
    "            \n",
    "# model.backbone.set_state_dict(_state)\n",
    "\n",
    "# paddle.save(_state, '../params/pretrain_mae_vit_base_mask_0.75_400e.params')\n",
    "\n",
    "# import torch\n",
    "# state = torch.load('../params/pretrain_mae_vit_base_mask_0.75_400e.pth', map_location='cpu')\n",
    "\n",
    "# for k, v  in state['model'].items():\n",
    "# #     if 'encoder' in k:\n",
    "# #         print(k.strip('encoder.'), v.shape)\n",
    "#     # print(k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backbone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############use_abs_pos: False\n",
      "############use_sincos_pos: True\n",
      "############use_rel_pos_bias: False\n",
      "##############patch here!!!\n",
      "##############patch here!!!\n",
      "##############self.pos_embed: Parameter containing:\n",
      "tensor([[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
      "         ...,\n",
      "         [-0.8555,  0.8573, -0.9248,  ...,  1.0000,  1.0000,  1.0000],\n",
      "         [-0.8555,  0.8573, -0.9248,  ...,  1.0000,  1.0000,  1.0000],\n",
      "         [-0.8555,  0.8573, -0.9248,  ...,  1.0000,  1.0000,  1.0000]]])\n"
     ]
    }
   ],
   "source": [
    "from vitpytorch import VisionTransformerDet\n",
    "\n",
    "tvit = VisionTransformerDet(img_size=[672, 1092], \n",
    "                     patch_size=16, \n",
    "                     embed_dim=768, \n",
    "                     depth=12, \n",
    "                     num_heads=12, \n",
    "                     init_values=0.1, \n",
    "                     mlp_ratio=4., \n",
    "                     drop_path_rate=0.2, \n",
    "                     drop_rate=0,\n",
    "                     use_abs_pos_emb=False, \n",
    "                     use_sincos_pos_emb=True, \n",
    "                     use_rel_pos_bias=False, \n",
    "                     use_checkpoint=False, \n",
    "                     out_indices=(3, 5, 7, 11),  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## paddle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from vitpaddle import VisionTransformer\n",
    "\n",
    "# pmodel = VisionTransformer(img_size=[672, 1092], \n",
    "#                      patch_size=16, \n",
    "#                      embed_dim=768, \n",
    "#                      depth=12, \n",
    "#                      num_heads=12, \n",
    "#                      init_values=0.1, \n",
    "#                      mlp_ratio=4., \n",
    "#                      drop_path_rate=0.2, \n",
    "#                      drop_rate=0,\n",
    "#                      use_abs_pos_emb=False, \n",
    "#                      # use_sincos_pos_emb=True, \n",
    "#                      use_rel_pos_bias=False, \n",
    "#                      use_checkpoint=False, \n",
    "#                      epsilon=1e-6,\n",
    "#                      out_indices=[3, 5, 7, 11],  )\n",
    "\n",
    "# pmodel = model.backbone\n",
    "\n",
    "pvit = model.backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================pos_embed static ================\n",
      "pos_embed torch.Size([1, 2857, 768])\n"
     ]
    }
   ],
   "source": [
    "pvit.eval()\n",
    "tvit.eval()\n",
    "\n",
    "# state = tvit.state_dict()\n",
    "_state = {}\n",
    "\n",
    "# keys_dict = {tk: pk for (tk, _), (pk, _) in zip(tvit.state_dict().items(), pvit.state_dict().items())}\n",
    "\n",
    "for n, p in tvit.state_dict().items():\n",
    "    if n == 'pos_embed':\n",
    "        print(n, p.shape)\n",
    "    \n",
    "    # _p = state[n] \n",
    "    _p = p.cpu().data.numpy()\n",
    "    \n",
    "    if len(_p.shape) == 2:\n",
    "        _p = _p.T\n",
    "    \n",
    "    if 'running_mean' in n:\n",
    "        n = n.replace('running_mean', '_mean')\n",
    "        \n",
    "    elif 'running_var' in n :\n",
    "        n = n.replace('running_var', '_variance')\n",
    "    \n",
    "    # _state[keys_dict[n]] = _p\n",
    "    \n",
    "    _state[n] = _p\n",
    "\n",
    "pvit.set_state_dict(_state)\n",
    "\n",
    "# paddle.save(_state, '../params/init.params')\n",
    "\n",
    "# for (tn, tv), (pn, pv) in zip(tvit.state_dict().items(), pvit.state_dict().items()):\n",
    "#     print(tn, tv.shape, pn, pv.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data = np.random.rand(1, 3, 160, 160).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 768, 40, 40] -8.88554350240156e-05 -109.18548583984375\n",
      "[1, 768, 20, 20] -0.002522312104701996 -774.854248046875\n",
      "[1, 768, 10, 10] 0.40181031823158264 30859.03125\n",
      "[1, 768, 5, 5] 0.7248580455780029 13917.2744140625\n"
     ]
    }
   ],
   "source": [
    "poutputs = pvit(paddle.to_tensor(data.copy()))\n",
    "# print(poutputs[-1])\n",
    "for out in poutputs:\n",
    "    print(out.shape, out.mean().item(), out.sum().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 768, 40, 40]) -8.885537681635469e-05 -109.18548583984375\n",
      "torch.Size([1, 768, 20, 20]) -0.002522312104701996 -774.854248046875\n",
      "torch.Size([1, 768, 10, 10]) 0.40181031823158264 30859.03125\n",
      "torch.Size([1, 768, 5, 5]) 0.7248581051826477 13917.275390625\n"
     ]
    }
   ],
   "source": [
    "tvit = tvit.cuda()\n",
    "toutputs = tvit(torch.tensor(data.copy()).cuda())\n",
    "# print(outputs[-1])\n",
    "\n",
    "for out in toutputs:\n",
    "    print(out.shape, out.mean().item(), out.sum().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HRFPN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     neck=dict(\n",
    "#         type='HRFPN',\n",
    "#         in_channels=[768, 768, 768, 768],\n",
    "#         out_channels=256,\n",
    "#         num_outs=5),\n",
    "\n",
    "# from ppdet.modeling.necks.hrfpn import HRFPN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cfg = load_config('./configs/cascade_rcnn/vit_base_16_hrfpn.yml')\n",
    "# neck = create(cfg.architecture).neck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from mmcv.cnn import ConvModule\n",
    "from mmcv.runner import BaseModule\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "\n",
    "class HRFPN(BaseModule):\n",
    "    '''https://github.com/open-mmlab/mmdetection/blob/master/mmdet/models/necks/hrfpn.py\n",
    "    '''\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 out_channels,\n",
    "                 num_outs=5,\n",
    "                 pooling_type='AVG',\n",
    "                 conv_cfg=None,\n",
    "                 norm_cfg=None,\n",
    "                 with_cp=False,\n",
    "                 stride=1,\n",
    "                 init_cfg=dict(type='Caffe2Xavier', layer='Conv2d')):\n",
    "        super(HRFPN, self).__init__(init_cfg)\n",
    "        assert isinstance(in_channels, list)\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.num_ins = len(in_channels)\n",
    "        self.num_outs = num_outs\n",
    "        self.with_cp = with_cp\n",
    "        self.conv_cfg = conv_cfg\n",
    "        self.norm_cfg = norm_cfg\n",
    "\n",
    "        self.reduction_conv = ConvModule(\n",
    "            sum(in_channels),\n",
    "            out_channels,\n",
    "            kernel_size=1,\n",
    "            conv_cfg=self.conv_cfg,\n",
    "            act_cfg=None)\n",
    "\n",
    "        self.fpn_convs = nn.ModuleList()\n",
    "        for i in range(self.num_outs):\n",
    "            self.fpn_convs.append(\n",
    "                ConvModule(\n",
    "                    out_channels,\n",
    "                    out_channels,\n",
    "                    kernel_size=3,\n",
    "                    padding=1,\n",
    "                    stride=stride,\n",
    "                    conv_cfg=self.conv_cfg,\n",
    "                    act_cfg=None))\n",
    "\n",
    "        if pooling_type == 'MAX':\n",
    "            self.pooling = F.max_pool2d\n",
    "        else:\n",
    "            self.pooling = F.avg_pool2d\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"Forward function.\"\"\"\n",
    "        assert len(inputs) == self.num_ins\n",
    "        outs = [inputs[0]]\n",
    "        \n",
    "        for i in range(1, self.num_ins):\n",
    "            outs.append(\n",
    "                F.interpolate(inputs[i], scale_factor=2**i, mode='bilinear'))\n",
    "            \n",
    "        out = torch.cat(outs, dim=1)\n",
    "        # print('out, ', out)\n",
    "\n",
    "        if out.requires_grad and self.with_cp:\n",
    "            out = checkpoint(self.reduction_conv, out)\n",
    "        else:\n",
    "            out = self.reduction_conv(out)\n",
    "            \n",
    "        # print('reduction_conv, ', out)\n",
    "\n",
    "        outs = [out]\n",
    "        for i in range(1, self.num_outs):\n",
    "            outs.append(self.pooling(out, kernel_size=2**i, stride=2**i))\n",
    "        outputs = []\n",
    "\n",
    "        for i in range(self.num_outs):\n",
    "            if outs[i].requires_grad and self.with_cp:\n",
    "                tmp_out = checkpoint(self.fpn_convs[i], outs[i])\n",
    "            else:\n",
    "                tmp_out = self.fpn_convs[i](outs[i])\n",
    "            outputs.append(tmp_out)\n",
    "        return tuple(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reduction_conv.conv.weight torch.Size([256, 3072, 1, 1])\n",
      "reduction_conv.conv.bias torch.Size([256])\n",
      "fpn_convs.0.conv.weight torch.Size([256, 256, 3, 3])\n",
      "fpn_convs.0.conv.bias torch.Size([256])\n",
      "fpn_convs.1.conv.weight torch.Size([256, 256, 3, 3])\n",
      "fpn_convs.1.conv.bias torch.Size([256])\n",
      "fpn_convs.2.conv.weight torch.Size([256, 256, 3, 3])\n",
      "fpn_convs.2.conv.bias torch.Size([256])\n",
      "fpn_convs.3.conv.weight torch.Size([256, 256, 3, 3])\n",
      "fpn_convs.3.conv.bias torch.Size([256])\n",
      "fpn_convs.4.conv.weight torch.Size([256, 256, 3, 3])\n",
      "fpn_convs.4.conv.bias torch.Size([256])\n",
      "-----\n",
      "reduction.weight [256, 3072, 1, 1]\n",
      "reduction.bias [256]\n",
      "fpn_conv_0.weight [256, 256, 3, 3]\n",
      "fpn_conv_0.bias [256]\n",
      "fpn_conv_1.weight [256, 256, 3, 3]\n",
      "fpn_conv_1.bias [256]\n",
      "fpn_conv_2.weight [256, 256, 3, 3]\n",
      "fpn_conv_2.bias [256]\n",
      "fpn_conv_3.weight [256, 256, 3, 3]\n",
      "fpn_conv_3.bias [256]\n",
      "fpn_conv_4.weight [256, 256, 3, 3]\n",
      "fpn_conv_4.bias [256]\n"
     ]
    }
   ],
   "source": [
    "tneck = HRFPN(in_channels=[768, 768, 768, 768], out_channels=256, num_outs=5).cuda()\n",
    "pneck = model.neck\n",
    "\n",
    "for n, p in tneck.state_dict().items():\n",
    "    print(n, p.shape)\n",
    "\n",
    "print('-----')\n",
    "for n, p in pneck.state_dict().items():\n",
    "    print(n, p.shape)\n",
    "    \n",
    "    \n",
    "# for (tn, tv), (pn, pv) in zip(tneck.state_dict().items(), pneck.state_dict().items()):\n",
    "#     print(tn, tv.shape, pn, pv.shape)\n",
    "\n",
    "\n",
    "pneck.eval()\n",
    "tneck.eval()\n",
    "\n",
    "\n",
    "_nect_state = {}\n",
    "keys_dict = {tk: pk for (tk, _), (pk, _) in zip(tneck.state_dict().items(), pneck.state_dict().items())}\n",
    "\n",
    "for n, p in tneck.state_dict().items():\n",
    "\n",
    "    _p = p.cpu().data.numpy()\n",
    "    \n",
    "    if len(_p.shape) == 2:\n",
    "        _p = _p.T\n",
    "    \n",
    "    if 'running_mean' in n:\n",
    "        n = n.replace('running_mean', '_mean')\n",
    "        \n",
    "    elif 'running_var' in n :\n",
    "        n = n.replace('running_var', '_variance')\n",
    "    \n",
    "    _nect_state[keys_dict[n]] = _p\n",
    "    \n",
    "    \n",
    "pneck.set_state_dict(_nect_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pneck.reduction.weight\n",
    "# tneck.reduction_conv.conv.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_neck_outputs = pneck(poutputs)\n",
    "t_neck_outputs = tneck(toutputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 256, 40, 40] 0.12531408667564392 51328.65625\n",
      "[1, 256, 20, 20] 0.21994656324386597 22522.53125\n",
      "[1, 256, 10, 10] 0.26829564571380615 6868.369140625\n",
      "[1, 256, 5, 5] 0.06000206619501114 384.01336669921875\n",
      "[1, 256, 2, 2] -0.1338818520307541 -137.0950164794922\n"
     ]
    }
   ],
   "source": [
    "for out in p_neck_outputs:\n",
    "    print(out.shape, out.mean().item(), out.sum().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 256, 40, 40]) 0.12531402707099915 51328.62890625\n",
      "torch.Size([1, 256, 20, 20]) 0.21994659304618835 22522.53125\n",
      "torch.Size([1, 256, 10, 10]) 0.26829567551612854 6868.36962890625\n",
      "torch.Size([1, 256, 5, 5]) 0.060002077370882034 384.0133056640625\n",
      "torch.Size([1, 256, 2, 2]) -0.1338818073272705 -137.094970703125\n"
     ]
    }
   ],
   "source": [
    "for out in t_neck_outputs:\n",
    "    print(out.shape, out.mean().item(), out.sum().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
