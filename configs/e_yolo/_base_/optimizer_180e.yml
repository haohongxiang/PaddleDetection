epoch: 180

LearningRate:
  base_lr: 0.01
  schedulers:
#  - !PiecewiseDecay
#    gamma: 0.1
#    milestones: [120]
  - !CosineDecay
    max_epochs: 216
#    eta_min: 0.0005
  - !LinearWarmup
    start_factor: 0.001
    steps: 2000

OptimizerBuilder:
  optimizer:
    momentum: 0.9
    type: Momentum
  regularizer:
    factor: 0.0005
    type: L2
